{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing_function.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1KuUhm9aKtSNNthP89ZKslkto7vGhl9Gw","authorship_tag":"ABX9TyPN6OeNVp+TEZKcW7TGUmFm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tz2SjwOtat_s","executionInfo":{"status":"ok","timestamp":1613493185661,"user_tz":-540,"elapsed":1457,"user":{"displayName":"â€ê¹€ì§€í›ˆ(í•™ë¶€í•™ìƒ/ê³µê³¼ëŒ€í•™ í† ëª©Â·í™˜ê²½ê³µí•™)","photoUrl":"","userId":"06228375687141397502"}},"outputId":"48a035b7-6ba6-441c-b0c6-b8443d1ff558"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import re\r\n","from datetime import datetime\r\n","from nltk.stem import WordNetLemmatizer\r\n","from nltk.corpus import stopwords\r\n","import nltk\r\n","nltk.download('stopwords')\r\n","nltk.download('punkt')\r\n","nltk.download('wordnet')\r\n","from nltk.tokenize import word_tokenize\r\n","from gensim.models import Word2Vec"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YAGniFuqfANS"},"source":["#ë‚˜ë¼ ìŠ¹ì¸ ì—¬ë¶€ë„ ì‹¤ì‹œê°„ ì—…ëƒ í• ê±°ë©´ ì´ê²ƒê¹Œì§€\r\n","#url = 'https://github.com/owid/covid-19-data/blob/master/public/data/vaccinations/locations.csv?raw=true'\r\n","#df = pd.read_csv(url)\r\n","#df['location'] = df['location'].str.lower()\r\n","#Pfizer_country = df[df['vaccines'].str.contains('Pfizer/BioNTech')].drop(['source_website','source_name'],axis=1)\r\n","#Moderna_country = df[df['vaccines'].str.contains('Moderna')].drop(['source_website','source_name'],axis=1)\r\n","#Pfizer_country['location'] = Pfizer_country['location'].str.lower()\r\n","#Moderna_country['location'] = Moderna_country['location'].str.lower()\r\n","#Pfizer_country.to_csv('/content/drive/MyDrive/DAá„á…µá†· /á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„á…©á„…á…©á„‚á…¡ á„‡á…¢á†¨á„‰á…µá†«/Pfizer_country.csv')\r\n","#Moderna_country.to_csv('/content/drive/MyDrive/DAá„á…µá†· /á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„á…©á„…á…©á„‚á…¡ á„‡á…¢á†¨á„‰á…µá†«/Moderna_country.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcSDttZHDn37","executionInfo":{"status":"ok","timestamp":1613493188476,"user_tz":-540,"elapsed":815,"user":{"displayName":"â€ê¹€ì§€í›ˆ(í•™ë¶€í•™ìƒ/ê³µê³¼ëŒ€í•™ í† ëª©Â·í™˜ê²½ê³µí•™)","photoUrl":"","userId":"06228375687141397502"}}},"source":["#í›„ì— ê²½ë¡œë³€ê²½ í•´ì•¼í•¨\r\n","Pfizer_country = pd.read_csv('/content/drive/MyDrive/DAá„á…µá†· /á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„á…©á„…á…©á„‚á…¡ á„‡á…¢á†¨á„‰á…µá†«/Pfizer_country.csv')\r\n","Moderna_country = pd.read_csv('/content/drive/MyDrive/DAá„á…µá†· /á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„á…©á„…á…©á„‚á…¡ á„‡á…¢á†¨á„‰á…µá†«/Moderna_country.csv')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"IURLlzZVOE6D","executionInfo":{"status":"ok","timestamp":1613493188776,"user_tz":-540,"elapsed":603,"user":{"displayName":"â€ê¹€ì§€í›ˆ(í•™ë¶€í•™ìƒ/ê³µê³¼ëŒ€í•™ í† ëª©Â·í™˜ê²½ê³µí•™)","photoUrl":"","userId":"06228375687141397502"}}},"source":["pfizer = pd.read_csv('/content/drive/MyDrive/DAá„á…µá†· /á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„á…©á„…á…©á„‚á…¡ á„‡á…¢á†¨á„‰á…µá†«/vaccination_tweets.csv')\r\n","moderna = pd.read_csv('/content/drive/MyDrive/DAá„á…µá†· /á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„á…©á„…á…©á„‚á…¡ á„‡á…¢á†¨á„‰á…µá†«/moderna_tweets_0214.csv')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBAUg_WvbBea","executionInfo":{"status":"ok","timestamp":1613493189323,"user_tz":-540,"elapsed":598,"user":{"displayName":"â€ê¹€ì§€í›ˆ(í•™ë¶€í•™ìƒ/ê³µê³¼ëŒ€í•™ í† ëª©Â·í™˜ê²½ê³µí•™)","photoUrl":"","userId":"06228375687141397502"}}},"source":["def Preprocessing(file):\r\n","  raw_data = file\r\n","  raw_data = raw_data.drop(['id','user_created','source'],axis=1)\r\n","  data1 = pd.DataFrame(raw_data[['user_name','user_location','user_followers','user_favourites','user_verified','favorites','is_retweet']])\r\n","  data2 = pd.DataFrame(raw_data['text'])\r\n","  #location\r\n","  data1['user_location'] = data1['user_location'].str.lower()\r\n","  data_co = data1.copy()\r\n","  for x in Pfizer_country['location']:\r\n","    for k in range(len(data_co)):\r\n","      if x in str(data_co['user_location'][k]):\r\n","        data_co['user_location'][k] = 1\r\n","  for k in range(len(data_co)):\r\n","    if 'london' in str(data_co['user_location'][k]):\r\n","      data_co['user_location'][k] = 1\r\n","  for k in range(len(data_co)):\r\n","    if 'uk' in str(data_co['user_location'][k]):\r\n","      data_co['user_location'][k] = 1\r\n","  for k in range(len(data_co)):\r\n","    if data_co['user_location'][k] != 1  :\r\n","      data_co['user_location'][k] = 0\r\n","  data1 = data_co\r\n","  #dropna\r\n","  data1 = data1[data1['is_retweet'].isnull()==False]\r\n","  data1 = data1.drop(['is_retweet'],axis=1)\r\n","  data1 = data1.reset_index(drop = True)\r\n","  #normalization\r\n","  #data1['user_followers']=float(data1['user_followers'])/float(max(data1['user_followers']))\r\n","  data1['user_favourites']=data1['user_favourites']/max(data1['user_favourites'])\r\n","  data1['favorites']=data1['favorites']/max(data1['favorites'])\r\n","  return data1, data2\r\n","  \r\n","\r\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQugphHBisiD","executionInfo":{"status":"ok","timestamp":1613493191597,"user_tz":-540,"elapsed":654,"user":{"displayName":"â€ê¹€ì§€í›ˆ(í•™ë¶€í•™ìƒ/ê³µê³¼ëŒ€í•™ í† ëª©Â·í™˜ê²½ê³µí•™)","photoUrl":"","userId":"06228375687141397502"}}},"source":["#Data2 ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜\r\n","def preprocess_tweet_data(data,name):\r\n","    # Lowering the case of the words in the sentences\r\n","    data[name]=data[name].str.lower()\r\n","    # Code to remove the Hashtags from the text\r\n","    data[name]=data[name].apply(lambda x:re.sub(r'#','',x))\r\n","    # Code to remove the links from the text\r\n","    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\r\n","    # Code to remove the Special characters from the text \r\n","    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\r\n","    # Code to substitute the multiple spaces with single spaces\r\n","    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\r\n","    # Code to remove all the single characters in the text\r\n","    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\r\n","    # Remove the twitter handlers\r\n","    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\r\n","def rem_stopwords_tokenize(data,name):\r\n","\r\n","    def getting(sen):\r\n","        example_sent = sen\r\n","\r\n","        stop_words = set(stopwords.words('english')) \r\n","\r\n","        word_tokens = word_tokenize(example_sent) \r\n","\r\n","        filtered_sentence = [w for w in word_tokens if not w in stop_words] \r\n","\r\n","        filtered_sentence = [] \r\n","\r\n","        for w in word_tokens: \r\n","            if w not in stop_words: \r\n","                filtered_sentence.append(w.lower()) \r\n","        return filtered_sentence\r\n","    x=[]\r\n","    for i in data[name].values:\r\n","        x.append(getting(i))\r\n","    data[name]=x\r\n","# Making a function to lemmatize all the words\r\n","lemmatizer = WordNetLemmatizer() \r\n","def lemmatize_all(data,name):\r\n","    arr=data[name]\r\n","    a=[]\r\n","    for i in arr:\r\n","        b=[]\r\n","        for j in i:\r\n","            x=lemmatizer.lemmatize(j,pos='a')\r\n","            x=lemmatizer.lemmatize(x)\r\n","            b.append(x)\r\n","        a.append(b)\r\n","    data[name]=a\r\n","# Function to make it back into a sentence \r\n","def make_sentences(data,name):\r\n","    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\r\n","    # Removing double spaces if created\r\n","    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\r\n","    "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zw9hrzirjdU","executionInfo":{"status":"ok","timestamp":1613493192958,"user_tz":-540,"elapsed":745,"user":{"displayName":"â€ê¹€ì§€í›ˆ(í•™ë¶€í•™ìƒ/ê³µê³¼ëŒ€í•™ í† ëª©Â·í™˜ê²½ê³µí•™)","photoUrl":"","userId":"06228375687141397502"}}},"source":["#Function to run all above\r\n","def preprocess_all(file):\r\n","  data1, data2 = Preprocessing(file)\r\n","  name = 'text'\r\n","  preprocess_tweet_data(data2,name)\r\n","  rem_stopwords_tokenize(data2,name)\r\n","  lemmatize_all(data2,name)\r\n","  make_sentences(data2,name)\r\n","  return data1, data2"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uiKrDlNBqrLZ","executionInfo":{"status":"ok","timestamp":1613493311101,"user_tz":-540,"elapsed":6947,"user":{"displayName":"â€ê¹€ì§€í›ˆ(í•™ë¶€í•™ìƒ/ê³µê³¼ëŒ€í•™ í† ëª©Â·í™˜ê²½ê³µí•™)","photoUrl":"","userId":"06228375687141397502"}},"outputId":"9f58cbec-d6f2-46a5-d082-bb3d4ced998f"},"source":["data1, data2 = preprocess_all(pfizer)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  from ipykernel import kernelapp as app\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":821},"id":"iwa10rm7rHmp","executionInfo":{"status":"ok","timestamp":1613493313725,"user_tz":-540,"elapsed":634,"user":{"displayName":"â€ê¹€ì§€í›ˆ(í•™ë¶€í•™ìƒ/ê³µê³¼ëŒ€í•™ í† ëª©Â·í™˜ê²½ê³µí•™)","photoUrl":"","userId":"06228375687141397502"}},"outputId":"2cc7c8bd-b9f4-4362-ef89-50e7b3f740f1"},"source":["display(data1)\r\n","display(data2)"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user_name</th>\n","      <th>user_location</th>\n","      <th>user_followers</th>\n","      <th>user_favourites</th>\n","      <th>user_verified</th>\n","      <th>favorites</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Rachel Roh</td>\n","      <td>0</td>\n","      <td>405</td>\n","      <td>0.003512</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Albert Fong</td>\n","      <td>0</td>\n","      <td>834</td>\n","      <td>0.000193</td>\n","      <td>False</td>\n","      <td>0.000432</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>eliğŸ‡±ğŸ‡¹ğŸ‡ªğŸ‡ºğŸ‘Œ</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>0.000168</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Charles Adler</td>\n","      <td>1</td>\n","      <td>49165</td>\n","      <td>0.023633</td>\n","      <td>True</td>\n","      <td>0.919654</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Citizen News Channel</td>\n","      <td>0</td>\n","      <td>152</td>\n","      <td>0.001593</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5371</th>\n","      <td>Marc Bailey ğŸ’™</td>\n","      <td>1</td>\n","      <td>759</td>\n","      <td>0.005298</td>\n","      <td>False</td>\n","      <td>0.001296</td>\n","    </tr>\n","    <tr>\n","      <th>5372</th>\n","      <td>SpaceNews Tech TsunamiHelp</td>\n","      <td>0</td>\n","      <td>427</td>\n","      <td>0.001530</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>5373</th>\n","      <td>Lib Dem Lords</td>\n","      <td>0</td>\n","      <td>13959</td>\n","      <td>0.001815</td>\n","      <td>True</td>\n","      <td>0.001296</td>\n","    </tr>\n","    <tr>\n","      <th>5374</th>\n","      <td>Scimos.org</td>\n","      <td>0</td>\n","      <td>56</td>\n","      <td>0.000031</td>\n","      <td>False</td>\n","      <td>0.001296</td>\n","    </tr>\n","    <tr>\n","      <th>5375</th>\n","      <td>Stephen</td>\n","      <td>0</td>\n","      <td>505</td>\n","      <td>0.006616</td>\n","      <td>False</td>\n","      <td>0.004320</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5376 rows Ã— 6 columns</p>\n","</div>"],"text/plain":["                       user_name user_location  ...  user_verified  favorites\n","0                     Rachel Roh             0  ...          False   0.000000\n","1                    Albert Fong             0  ...          False   0.000432\n","2                       eliğŸ‡±ğŸ‡¹ğŸ‡ªğŸ‡ºğŸ‘Œ             0  ...          False   0.000000\n","3                  Charles Adler             1  ...           True   0.919654\n","4           Citizen News Channel             0  ...          False   0.000000\n","...                          ...           ...  ...            ...        ...\n","5371               Marc Bailey ğŸ’™             1  ...          False   0.001296\n","5372  SpaceNews Tech TsunamiHelp             0  ...          False   0.000000\n","5373               Lib Dem Lords             0  ...           True   0.001296\n","5374                  Scimos.org             0  ...          False   0.001296\n","5375                     Stephen             0  ...          False   0.004320\n","\n","[5376 rows x 6 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>folk said daikon paste could treatcytokine sto...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>world wrong side history year hopefully big va...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>coronavirus sputnikv astrazeneca pfizerbiontec...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>fact immutable senator even ethically sturdy e...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>explain needvaccine borisjohnson matthancock w...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5371</th>\n","      <td>salmondleeds thanksm counting pfizerbiontech p...</td>\n","    </tr>\n","    <tr>\n","      <th>5372</th>\n","      <td>covid19 vaccine best reaction pfizer thanks ja...</td>\n","    </tr>\n","    <tr>\n","      <th>5373</th>\n","      <td>lord paulscriven asks reassurance people recei...</td>\n","    </tr>\n","    <tr>\n","      <th>5374</th>\n","      <td>part 2 covid vaccine series moderna pfizerbion...</td>\n","    </tr>\n","    <tr>\n","      <th>5375</th>\n","      <td>2 2 done got second dose pfizer vaccine chuffe...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5376 rows Ã— 1 columns</p>\n","</div>"],"text/plain":["                                                   text\n","0     folk said daikon paste could treatcytokine sto...\n","1     world wrong side history year hopefully big va...\n","2     coronavirus sputnikv astrazeneca pfizerbiontec...\n","3     fact immutable senator even ethically sturdy e...\n","4     explain needvaccine borisjohnson matthancock w...\n","...                                                 ...\n","5371  salmondleeds thanksm counting pfizerbiontech p...\n","5372  covid19 vaccine best reaction pfizer thanks ja...\n","5373  lord paulscriven asks reassurance people recei...\n","5374  part 2 covid vaccine series moderna pfizerbion...\n","5375  2 2 done got second dose pfizer vaccine chuffe...\n","\n","[5376 rows x 1 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"9QG5RvrZr5JB"},"source":[""],"execution_count":null,"outputs":[]}]}